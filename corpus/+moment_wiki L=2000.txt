in mathematics a moment is a specific quantitative measure used in both mechanics and statistics of the shape of a set of points if the points represent mass then the zeroth moment is the total mass the first moment divided by the total mass is the center of mass and the second moment is the rotational inertia if the points represent probability density then the zeroth moment is the total probability ie one the first moment is the mean the second central moment is the variance the third central moment is the skewness and the fourth central moment with normalization and shift is the kurtosis the mathematical concept is closely related to the concept of moment in physics for a distribution of mass or probability on a bounded interval the collection of all the moments of all orders from to uniquely determines the distribution hausdorff moment problem the same is not true on unbounded intervals hamburger moment problem significance of the moments the n-th moment of a real-valued continuous function fx of a real variable about a value c is n x c n f x d x displaystyle mu nint infty x-cnfxmathrm d x it is possible to define moments for random variables in a more general fashion than moments for real valuessee moments in metric spaces the moment of a function without further explanation usually refers to the above expression with c for the second and higher moments the central moment moments about the mean with c being the mean are usually used rather than the moments about zero because they provide clearer information about the distribution's shape other moments may also be defined for example the n-th inverse moment about zero is e x n displaystyle operatorname e leftx-nright and the n-th logarithmic moment about zero is e ln n x displaystyle operatorname e leftln nxright the n-th moment about zero of a probability density function fx is the expected value of xn and is called a raw moment or crude moment the moments about its mean are called central moments these describe the shape of the function independently of translation if f is a probability density function then the value of the integral above is called the n-th moment of the probability distribution more generally if f is a cumulative probability distribution function of any probability distribution which may not have a density function then the n-th moment of the probability distribution is given by the riemannstieltjes integral n e x n x n d f x displaystyle mu e leftxnrightint infty xnmathrm d fx where x is a random variable that has this cumulative distribution f and e is the expectation operator or mean when e x n x n d f x displaystyle operatorname e leftleftxnrightrightint infty leftxnrightmathrm d fxinfty then the moment is said not to exist if the n-th moment about any point exists so does the n moment and thus all lower-order moments about every point the zeroth moment of any probability density function is since the area under any probability density function must be equal to one significance of moments raw central normalised and cumulants raw normalised in connection with named properties of distributions moment ordinal moment cumulant raw central normalised raw standardised mean mean na variance variance skewness skewness non-excess or historical kurtosis excess kurtosis hyperskewness hyperflatness mean main article mean the first raw moment is the mean usually denoted e x displaystyle mu equiv operatorname e x variance main article variance the second central moment is the variance the positive square root of the variance is the standard deviation e x displaystyle sigma equiv leftoperatorname e leftx-mu rightrightfrac normalised moments main article standardized moment the normalised n-th central moment or standardised moment is the n-th central moment divided by n the normalised n-th central moment of the random variable x is n n e x n n displaystyle frac mu nsigma nfrac operatorname e leftx-mu nrightsigma n these normalised central moments are dimensionless quantities which represent the distribution independently of any linear change of scale for an electric signal the first moment is its dc level and the nd moment is proportional to its average power skewness main article skewness the third central moment is the measure of the lopsidedness of the distribution any symmetric distribution will have a third central moment if defined of zero the normalised third central moment is called the skewness often a distribution that is skewed to the left the tail of the distribution is longer on the left will have a negative skewness a distribution that is skewed to the right the tail of the distribution is longer on the right will have a positive skewness for distributions that are not too different from the normal distribution the median will be somewhere near the mode about kurtosis main article kurtosis the fourth central moment is a measure of the heaviness of the tail of the distribution compared to the normal distribution of the same variance since it is the expectation of a fourth power the fourth central moment where defined is always nonnegative and except for a point distribution it is always strictly positive the fourth central moment of a normal distribution is the kurtosis is defined to be the normalised fourth central moment minus equivalently as in the next section it is the fourth cumulant divided by the square of the variance some authorities do not subtract three but it is usually more convenient to have the normal distribution at the origin of coordinates if a distribution has heavy tails the kurtosis will be high sometimes called leptokurtic conversely light-tailed distributions for example bounded distributions such as the uniform have low kurtosis sometimes called platykurtic the kurtosis can be positive without limit but must be greater than or equal to equality only holds for binary distributions for unbounded skew distributions not too far from normal tends to be somewhere in the area of and the inequality can be proven by considering e t a t displaystyle operatorname e leftt-at-right where t x this is the expectation of a square so it is non-negative for all a however it is also a quadratic polynomial in a its discriminant must be non-positive which gives the required relationship mixed moments mixed moments are moments involving multiple variables some examples are covariance coskewness and cokurtosis while there is a unique covariance there are multiple co-skewnesses and co-kurtoses higher moments high-order moments are moments beyond th-order moments as with variance skewness and kurtosis these are higher-order statistics involving non-linear combinations of the data and can be used for description or estimation of further shape parameters the higher the moment the harder it is to estimate in the sense that larger samples are required in order to obtain estimates of similar quality this is due to the excess degrees of freedom consumed by the higher orders further they can be subtle to interpret often being most easily understood in terms of lower order moments compare the higher derivatives of jerk and jounce in physics for example just as the th-order moment kurtosis can be interpreted as relative importance of tails versus shoulders in causing dispersion for a given dispersion high kurtosis corresponds to heavy tails while low kurtosis corresponds to broad shoulders the th-order moment can be interpreted as measuring relative importance of tails versus center mode shoulders in causing skew for a given skew high th moment corresponds to heavy tail and little movement of mode while low th moment corresponds to more change in shoulders transformation of center since x b n x a a b n i n n i x a i a b n i displaystyle x-bnx-aa-bnsum inn choose ix-aia-bn-i where n i displaystyle dbinom ni is the binomial coefficient it follows that the moments about b can be calculated from the moments about a by e x b n i n n i e x a i a b n i displaystyle eleftx-bnrightsum inn choose ieleftx-airighta-bn-i cumulants main article cumulant the first raw moment and the second and third unnormalized central moments are additive in the sense that if x and y are independent random variables then m x y m x m y var x y var x var y x y x y displaystyle beginalignedmxymxmyoperatorname var xyoperatorname var xoperatorname var ymu xymu xmu yendaligned these can also hold for variables that satisfy weaker conditions than independence the first always holds if the second holds the variables are called uncorrelated in fact these are the first three cumulants and all cumulants share this additivity property sample moments for all k the k-th raw moment of a population can be estimated using the k-th raw sample moment n i n x i k displaystyle frac nsum inxik applied to a sample x xn drawn from the population it can be shown that the expected value of the raw sample moment is equal to the k-th raw moment of the population if that moment exists for any sample size n it is thus an unbiased estimator this contrasts with the situation for central moments whose computation uses up a degree of freedom by using the sample mean so for example an unbiased estimate of the population variance the second central moment is given by n i n x i x displaystyle frac n-sum inleftxi-bar xright in which the previous denominator n has been replaced by the degrees of freedom n and in which x displaystyle bar x refers to the sample mean this estimate of the population moment is greater than the unadjusted observed sample moment by a factor of n n displaystyle tfrac nn- and it is referred to as the adjusted sample variance or sometimes simply the sample variance problem of moments main article moment problem the problem of moments seeks characterizations of sequences n  n that are sequences of moments of some function f partial moments partial moments are sometimes referred to as one-sided moments the n-th order lower and upper partial moments with respect to a reference point r may be expressed as n r r r x n f x d x displaystyle mu n-rint rr-xnfxmathrm d x n r r x r n f x d x displaystyle mu nrint rinfty x-rnfxmathrm d x partial moments are normalized by being raised to the power n the upside potential ratio may be expressed as a ratio of a first-order upper partial moment to a normalized second-order lower partial moment they have been used in the definition of some financial metrics such as the sortino ratio as they focus purely on upside or downside central moments in metric spaces let m d be a metric space and let bm be the borel on m the generated by the d-open subsets of m for technical reasons it is also convenient to assume that m is a separable space with respect to the metric d let p the pth central moment of a measure on the measurable space m bm about a given point x m is defined to be m d x x p d x displaystyle int mdleftxxrightpmathrm d mu x is said to have finite p-th central moment if the p-th central moment of about x is finite for some x m this terminology for measures carries over to random variables in the usual way if p is a probability space and x  m is a random variable then the p-th central moment of x about x m is defined to be m d x x p d x p x d x x p d p displaystyle int mdxxpmathrm d leftxmathbf p rightxequiv int omega dleftxomega xrightpmathrm d mathbf p omega and x has finite p-th central moment if the p-th central moment of x about x is finite for some x m 
